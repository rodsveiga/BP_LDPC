{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding with Belief Propagation\n",
    "\n",
    "We want to iterate the following the following equations:\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu } \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$  \n",
    "\n",
    "which are Eqs.(96) from the paper [Low-density parity-check codesâ€”A statistical physics perspective](https://www.sciencedirect.com/science/article/pii/S1076567002800180), by R. Vicente, D. Saad and Y. Kabashima.\n",
    "\n",
    "The function $ \\beta(x) $ is the Nishimori temperature,\n",
    "\n",
    "$$ \\beta(x) = \\frac{1}{2} \\log \\left(  \\frac{1- \\rho}{\\rho}  \\right) \\; \\; .  $$\n",
    "\n",
    "The quantity $\\rho$ is the flip probablility of the noisy channel (BSC),\n",
    "\n",
    "$$ P ( J | J^{(0)} ) = (1 - \\rho) \\delta_{J, J^{(0)} } + \\rho \\delta_{J, -J^{(0)} }   \\; \\; , $$\n",
    "\n",
    "whereas the prior distribution for each meassage bit is assumed to be\n",
    "\n",
    "$$ P ( S_j ) = (1 - \\rho_\\xi) \\delta_{+1, S_j }  +  \\rho_\\xi \\delta_{-1, S_j }  \\; \\; .  $$\n",
    "\n",
    "The object $ {\\cal L} (\\mu)$ represents the set of $K$ non-zero elements on the row $\\mu$ of the code generator matrix ${\\cal G}$ (the one which adds redundancy), \n",
    "\n",
    "$$  {\\cal L} (\\mu) = \\langle i_1, i_2, ..., i_K \\rangle  \\; \\; .  $$\n",
    "\n",
    "The are $C$ non-zero elements per column on the matrix ${\\cal G}$:\n",
    "\n",
    "$$ \\sum_{\\mu : j \\in {\\cal L}(\\mu)} i_j = C \\; \\; ; \\; \\; \\forall j = 1, ..., K \\; \\; . $$\n",
    "\n",
    "The object $ {\\cal M} (j)$ represents the set of all index sets that contain $j$.\n",
    "\n",
    "After convergence of the iterative procedure, we can calculate the pseudo-posterior:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$\n",
    "from which the Bayes optimal estimate is obtained\n",
    "$$ \\hat{\\xi}_j  = sign( m_j ) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the variables of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message lengh\n",
    "N = 100\n",
    "\n",
    "# Codeword lengh\n",
    "M = 200\n",
    "\n",
    "# Non-zero elements per row of the generation matrix\n",
    "K = 4\n",
    "\n",
    "# Number of messages\n",
    "n = 10\n",
    "\n",
    "# Noisy channel\n",
    "p = 0.5\n",
    "beta = 0.5*np.log( (1 - p) / p)\n",
    "\n",
    "# Message prior\n",
    "p_prior = 0.1\n",
    "beta_prior = 0.5*np.log( (1 - p_prior) / p_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating messages\n",
    "\n",
    "Each message is a $N$ dimensional vector. Generate a set of $n$ messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand([n, N])\n",
    "    \n",
    "message = torch.zeros([n, N])\n",
    "\n",
    "for j in range(random.shape[0]):\n",
    "    \n",
    "    for k in range(random.shape[1]):\n",
    "               \n",
    "            #### -1 with probability p_prior\n",
    "            if random[j,k] <= p_prior:\n",
    "                message[j,k] = -1.\n",
    "            #### +1 with probability 1 - p_prior\n",
    "            else:\n",
    "                message[j,k] = +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Each message is encoded to a high dimensional vector ${\\bf J}^{(0)} \\in \\{ \\pm 1  \\}^M$ defined as \n",
    "\n",
    "$$   J^{(0)}_{\\langle i_1, i_2, ...., i_K \\rangle} = \\xi_1 \\xi_ 2 ... \\xi_K  \\; \\; ,$$\n",
    "\n",
    "where $M$ sets of $K \\in [ 1, ..., N]$ indexes are randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.randint(0, N, [M, K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `encoding`, we construct the encoded message ${\\bf J}^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for j in range(message.shape[0]):\n",
    "    \n",
    "#    J0 = torch.take(message[j], encoding).prod(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "J0 = torch.take(message[0], encoding).prod(dim=1)\n",
    "J0 = J0.unsqueeze(0)\n",
    "\n",
    "for j in range(1, message.shape[0]):\n",
    "    \n",
    "    J0_ = torch.take(message[j], encoding).prod(dim=1)\n",
    "    J0_ = J0_.unsqueeze(0)\n",
    "    \n",
    "    J0 = torch.cat((J0, J0_), dim= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupted version of the encoded set of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand(J.shape)\n",
    "                      \n",
    "for j in range(J.shape[0]):\n",
    "    for k in range(J.shape[1]):\n",
    "          \n",
    "        if random[j, k] <= p:\n",
    "            J[j, k] = -J[j, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
       "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
       "        [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
       "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
       "        [ 1., -1., -1.,  ...,  1., -1., -1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating BP equations for one message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus in one received message to iterate the belief propagation equations.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)    \\right)   \\; \\; ,  $$  \n",
    "\n",
    "with $j = 1, ..., N$ and $\\mu = 1, ..., M$.\n",
    "\n",
    "We cal this message `J_`. We will worry later about a loop over all the received messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
      "         1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
      "        -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
      "        -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
      "        -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
      "        -1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
      "         1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
      "        -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "         1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
      "         1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
      "         1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
      "         1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
      "         1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
      "        -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,\n",
      "         1., -1.,  1., -1.])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "J_ = J[0]\n",
    "print(J_)\n",
    "print(J_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization of the beliefs $m_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2830, 0.1804, 0.0283,  ..., 0.8417, 0.7593, 0.6501],\n",
       "        [0.4263, 0.6466, 0.4386,  ..., 0.8231, 0.2743, 0.2494],\n",
       "        [0.7610, 0.6929, 0.3876,  ..., 0.3168, 0.2898, 0.5129],\n",
       "        ...,\n",
       "        [0.1362, 0.8182, 0.2399,  ..., 0.7423, 0.7903, 0.2556],\n",
       "        [0.7293, 0.5451, 0.6052,  ..., 0.1215, 0.8149, 0.5406],\n",
       "        [0.1735, 0.7988, 0.3331,  ..., 0.9662, 0.1540, 0.7476]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty tensor to represent $\\hat{m}_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat = torch.empty(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0823e+23, 4.2186e-08, 2.6807e-09,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate $\\hat{m}_{\\mu j}$.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "This first implementation has two `for` loops. This is potentially harmful if one cares about efficienty. We obviously do, but since we are just beginning, lets go on like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu in range(M):\n",
    "    for j in range(N):\n",
    "          \n",
    "        # Keep only L(mu) which a are different of j\n",
    "        index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "        L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "        # Message update        \n",
    "        m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [-0., -0., -0.,  ..., -0., -0., -0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement:\n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)     \\right)  \\; \\; ,  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(N):\n",
    "    \n",
    "    for mu in range(M):\n",
    "        \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "        M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "        \n",
    "        m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        ...,\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the pseudo-posterior, which is calculated after convergence of the messages above:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_bayes = []\n",
    "\n",
    "for j in range(N):\n",
    "    \n",
    "    M_set = torch.where(encoding == j)[0]\n",
    "       \n",
    "    m_j = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "    m_bayes.append(m_j.item())\n",
    "            \n",
    "m_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(m_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering all message iterations on functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_to_mhat(N, M, J_, m, beta, encoding):\n",
    "    \n",
    "    m_hat = torch.empty(M, N)\n",
    "    \n",
    "    for mu in range(M):\n",
    "        for j in range(N):\n",
    "                     \n",
    "            # Keep only L(mu) which a are different of j\n",
    "            index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "            L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "            # Message update        \n",
    "            m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)\n",
    "            \n",
    "    return m_hat\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def mhat_to_m(N, M, m_hat, beta_prior, encoding):\n",
    "    \n",
    "    m = torch.empty(M, N)\n",
    "    \n",
    "    for j in range(N):\n",
    "        \n",
    "        for mu in range(M):\n",
    "            \n",
    "            M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "            M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "            \n",
    "            m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)\n",
    "            \n",
    "    return m\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def m_bayes(N, m_hat_final, beta_prior, encoding):\n",
    "    \n",
    "    m_bayes = []\n",
    "\n",
    "    for j in range(N):\n",
    "           \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "  \n",
    "        m_j = torch.tanh(torch.take(np.arctanh(m_hat_final[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "        m_bayes.append(m_j.item())\n",
    "        \n",
    "    return m_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a function for the iterative decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP_LDPC(N, M, J_, beta, beta_prior, encoding, message, num_it= 100, verbose= 1):\n",
    "     \n",
    "    m_hat = torch.rand(M, N)\n",
    "    m = torch.empty(M, N)\n",
    "    \n",
    "    for j in range(num_it):\n",
    "        \n",
    "        print('--it = %d' % j)\n",
    "              \n",
    "        m = mhat_to_m(N, M, m_hat, beta_prior, encoding)\n",
    "        m_hat = m_to_mhat(N, M, J_, m, beta, encoding)\n",
    "        \n",
    "        ## Monitoring performace\n",
    "        if (j % verbose) == 0:\n",
    "            m_ = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "            print('overlap= ', torch.dot(message, torch.Tensor(np.sign(m_))).item() / N)\n",
    "        \n",
    "    m_final = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "    \n",
    "    return np.sign(m_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--it = 0\n",
      "overlap=  0.76\n",
      "--it = 1\n",
      "overlap=  0.76\n",
      "--it = 2\n",
      "overlap=  0.76\n",
      "--it = 3\n",
      "overlap=  0.76\n",
      "--it = 4\n",
      "overlap=  0.76\n",
      "--it = 5\n",
      "overlap=  0.76\n",
      "--it = 6\n",
      "overlap=  0.76\n",
      "--it = 7\n",
      "overlap=  0.76\n",
      "--it = 8\n",
      "overlap=  0.76\n",
      "--it = 9\n",
      "overlap=  0.76\n",
      "--it = 10\n",
      "overlap=  0.76\n",
      "--it = 11\n",
      "overlap=  0.76\n",
      "--it = 12\n",
      "overlap=  0.76\n",
      "--it = 13\n",
      "overlap=  0.76\n",
      "--it = 14\n",
      "overlap=  0.76\n",
      "--it = 15\n",
      "overlap=  0.76\n",
      "--it = 16\n",
      "overlap=  0.76\n",
      "--it = 17\n",
      "overlap=  0.76\n",
      "--it = 18\n",
      "overlap=  0.76\n",
      "--it = 19\n",
      "overlap=  0.76\n"
     ]
    }
   ],
   "source": [
    "opt_dec_Bayes = BP_LDPC(N, M, J_, beta, beta_prior, encoding, message[0], num_it= 20, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. It is still slow (we will worrie about this latter), but it appears to work.\n",
    "\n",
    "Observe that we arbitrarily consider a certain `num_it`. Naturally, a important question remains: how do we monitor the convergence of BP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test on previous codes and messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message lengh\n",
    "N = 250\n",
    "\n",
    "# Codeword lengh\n",
    "M = 500\n",
    "\n",
    "# Non-zero elements per row of the generation matrix\n",
    "K = 4\n",
    "\n",
    "# Number of messages\n",
    "n = 100\n",
    "\n",
    "# Noisy channel\n",
    "p = 0.3\n",
    "beta = 0.5*np.log( (1 - p) / p)\n",
    "\n",
    "# Message prior\n",
    "p_prior = 0.01\n",
    "beta_prior = 0.5*np.log( (1 - p_prior) / p_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.load('codes/code_N_250_M_500_K_4_p_03_p_prior_001_j_0.pt')\n",
    "message = torch.load('codes/message_N_250_M_500_K_4_p_03_p_prior_001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        ...,\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 250])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(message)\n",
    "message.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we made a mistake saving the codes. We have actually saved the messages. That is a problem. For now, let us generate a new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 94,  21, 204, 242],\n",
       "        [162,  49, 100, 151],\n",
       "        [230, 198, 123,  57],\n",
       "        ...,\n",
       "        [ 87, 176,   1, 211],\n",
       "        [ 99,  72,  51,  91],\n",
       "        [248, 145, 105, 154]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding  = torch.randint(0, N, [M, K])\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "J0 = torch.take(message[0], encoding).prod(dim=1)\n",
    "J0 = J0.unsqueeze(0)\n",
    "\n",
    "for j in range(1, message.shape[0]):\n",
    "    \n",
    "    J0_ = torch.take(message[j], encoding).prod(dim=1)\n",
    "    J0_ = J0_.unsqueeze(0)\n",
    "    \n",
    "    J0 = torch.cat((J0, J0_), dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 500])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J0)\n",
    "J0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrupted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J0.clone()\n",
    "\n",
    "random = torch.rand(J.shape)\n",
    "                      \n",
    "for j in range(J.shape[0]):\n",
    "    for k in range(J.shape[1]):\n",
    "          \n",
    "        if random[j, k] <= p:\n",
    "            J[j, k] = -J[j, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 500])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "500\n",
      "torch.Size([500, 4])\n",
      "torch.Size([100, 500])\n",
      "torch.Size([100, 250])\n"
     ]
    }
   ],
   "source": [
    "print(N)\n",
    "print(M)\n",
    "print(encoding.shape)\n",
    "print(J.shape)\n",
    "print(message.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_list = []\n",
    "\n",
    "for k in range(int(J.shape[0] / 10 )):\n",
    "    \n",
    "    print('----message %d' % k)\n",
    "       \n",
    "    opt_dec_Bayes = BP_LDPC(N, M, J[k], beta, beta_prior, encoding, message[k], num_it= 10, verbose= 1)\n",
    "    \n",
    "    overlap_list.append(opt_dec_Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
