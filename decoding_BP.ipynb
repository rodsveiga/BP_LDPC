{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding with Belief Propagation\n",
    "\n",
    "We want to iterate the following the following equations:\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu } \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$  \n",
    "\n",
    "which are Eqs.(96) from the paper [Low-density parity-check codesâ€”A statistical physics perspective](https://www.sciencedirect.com/science/article/pii/S1076567002800180), by R. Vicente, D. Saad and Y. Kabashima.\n",
    "\n",
    "The function $ \\beta(x) $ is the Nishimori temperature,\n",
    "\n",
    "$$ \\beta(x) = \\frac{1}{2} \\log \\left(  \\frac{1- \\rho}{\\rho}  \\right) \\; \\; .  $$\n",
    "\n",
    "The quantity $\\rho$ is the flip probablility of the noisy channel (BSC),\n",
    "\n",
    "$$ P ( J | J^{(0)} ) = (1 - \\rho) \\delta_{J, J^{(0)} } + \\rho \\delta_{J, -J^{(0)} }   \\; \\; , $$\n",
    "\n",
    "whereas the prior distribution for each meassage bit is assumed to be\n",
    "\n",
    "$$ P ( S_j ) = (1 - \\rho_\\xi) \\delta_{+1, S_j }  +  \\rho_\\xi \\delta_{-1, S_j }  \\; \\; .  $$\n",
    "\n",
    "The object $ {\\cal L} (\\mu)$ represents the set of $K$ non-zero elements on the row $\\mu$ of the code generator matrix ${\\cal G}$ (the one which adds redundancy), \n",
    "\n",
    "$$  {\\cal L} (\\mu) = \\langle i_1, i_2, ..., i_K \\rangle  \\; \\; .  $$\n",
    "\n",
    "The are $C$ non-zero elements per column on the matrix ${\\cal G}$:\n",
    "\n",
    "$$ \\sum_{\\mu : j \\in {\\cal L}(\\mu)} i_j = C \\; \\; ; \\; \\; \\forall j = 1, ..., K \\; \\; . $$\n",
    "\n",
    "The object $ {\\cal M} (j)$ represents the set of all index sets that contain $j$.\n",
    "\n",
    "After convergence of the iterative procedure, we can calculate the pseudo-posterior:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$\n",
    "from which the Bayes optimal estimate is obtained\n",
    "$$ \\hat{\\xi}_j  = sign( m_j ) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the variables of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message lengh\n",
    "N = 100\n",
    "\n",
    "# Codeword lengh\n",
    "M = 200\n",
    "\n",
    "# Non-zero elements per row of the generation matrix\n",
    "K = 4\n",
    "\n",
    "# Number of messages\n",
    "n = 10\n",
    "\n",
    "# Noisy channel\n",
    "p = 0.3\n",
    "beta = 0.5*np.log( (1 - p) / p)\n",
    "\n",
    "# Message prior\n",
    "p_prior = 0.1\n",
    "beta_prior = 0.5*np.log( (1 - p_prior) / p_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating messages\n",
    "\n",
    "Each message is a $N$ dimensional vector. Generate a set of $n$ messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand([n, N])\n",
    "    \n",
    "message = torch.zeros([n, N])\n",
    "\n",
    "for j in range(random.shape[0]):\n",
    "    \n",
    "    for k in range(random.shape[1]):\n",
    "               \n",
    "            #### -1 with probability p_prior\n",
    "            if random[j,k] <= p_prior:\n",
    "                message[j,k] = -1.\n",
    "            #### +1 with probability 1 - p_prior\n",
    "            else:\n",
    "                message[j,k] = +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each message is encoded to a high dimensional vector ${\\bf J}^{(0)} \\in \\{ \\pm 1  \\}^M$ defined as \n",
    "\n",
    "$$   J^{(0)}_{\\langle i_1, i_2, ...., i_K \\rangle} = \\xi_1 \\xi_ 2 ... \\xi_K  \\; \\; ,$$\n",
    "\n",
    "where $M$ sets of $K \\in [ 1, ..., N]$ indexes are randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.randint(0, N, [M, K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 4])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `encoding`, we construct the encoded message ${\\bf J}^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(message.shape[0]):\n",
    "    \n",
    "    J0 = torch.take(message[j], encoding).prod(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "J0 = torch.take(message[0], encoding).prod(dim=1)\n",
    "J0 = J0.unsqueeze(0)\n",
    "\n",
    "for j in range(1, message.shape[0]):\n",
    "    \n",
    "    J0_ = torch.take(message[j], encoding).prod(dim=1)\n",
    "    J0_ = J0_.unsqueeze(0)\n",
    "    \n",
    "    J0 = torch.cat((J0, J0_), dim= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the corrupted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand(J.shape)\n",
    "                      \n",
    "for j in range(J.shape[0]):\n",
    "    for k in range(J.shape[1]):\n",
    "          \n",
    "        if random[j, k] <= p:\n",
    "            J[j, k] = -J[j, k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus in one received message to iterate the belief propagation equations.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)    \\right)   \\; \\; ,  $$  \n",
    "\n",
    "with $j = 1, ..., N$ and $\\mu = 1, ..., M$.\n",
    "\n",
    "We cal this message `J_`. We will worry later about a loop over all the received messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
      "         1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
      "         1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
      "         1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
      "         1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
      "         1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
      "         1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,\n",
      "         1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
      "         1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
      "        -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
      "         1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
      "         1.,  1.,  1.,  1.])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "J_ = J[0]\n",
    "print(J_)\n",
    "print(J_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization of the beliefs $m_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5737, 0.3259, 0.1629,  ..., 0.5391, 0.5564, 0.4237],\n",
       "        [0.8206, 0.2177, 0.8888,  ..., 0.0783, 0.1224, 0.3776],\n",
       "        [0.6112, 0.9259, 0.9299,  ..., 0.8411, 0.8729, 0.6649],\n",
       "        ...,\n",
       "        [0.5682, 0.4077, 0.6779,  ..., 0.6039, 0.6576, 0.7261],\n",
       "        [0.2614, 0.4347, 0.6711,  ..., 0.1953, 0.9119, 0.3932],\n",
       "        [0.4544, 0.2197, 0.0443,  ..., 0.8243, 0.3393, 0.3847]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty tensor to represent $\\hat{m}_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat = torch.empty(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate $\\hat{m}_{\\mu j}$.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "This first implementation has two `for` loops. This is potentially harmful if one cares about efficienty. We obviously do, but since we are just beginning, lets go on like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu in range(M):\n",
    "    for j in range(N):\n",
    "          \n",
    "        # Keep only L(mu) which a are different of j\n",
    "        index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "        L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "        # Message update        \n",
    "        m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2171e-06,  5.2171e-06,  5.2171e-06,  ...,  5.2171e-06,\n",
       "          5.2171e-06,  5.2171e-06],\n",
       "        [ 2.0069e-02,  2.0069e-02,  2.0069e-02,  ...,  2.0069e-02,\n",
       "          2.0069e-02,  2.0069e-02],\n",
       "        [-2.1374e-02, -2.1374e-02, -2.1374e-02,  ..., -2.1374e-02,\n",
       "         -2.1374e-02, -2.1374e-02],\n",
       "        ...,\n",
       "        [ 1.1966e-03,  1.1966e-03,  1.1966e-03,  ...,  1.1966e-03,\n",
       "          1.1966e-03,  1.1966e-03],\n",
       "        [ 2.8509e-03,  2.8509e-03,  2.8509e-03,  ...,  2.8509e-03,\n",
       "          2.8509e-03,  2.8509e-03],\n",
       "        [ 1.1827e-02,  1.1827e-02,  1.1827e-02,  ...,  1.4347e-02,\n",
       "          1.1827e-02,  1.1827e-02]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement:\n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)     \\right)  \\; \\; ,  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(N):\n",
    "    \n",
    "    for mu in range(M):\n",
    "        \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "        M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "        \n",
    "        m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8080, 0.6590, 0.7828,  ..., 0.9041, 0.6813, 0.7584],\n",
       "        [0.8080, 0.6590, 0.7828,  ..., 0.9041, 0.6813, 0.7584],\n",
       "        [0.8080, 0.6590, 0.7828,  ..., 0.9041, 0.6813, 0.7584],\n",
       "        ...,\n",
       "        [0.8080, 0.6590, 0.7828,  ..., 0.9041, 0.6813, 0.7584],\n",
       "        [0.8080, 0.6590, 0.7828,  ..., 0.9041, 0.6813, 0.7584],\n",
       "        [0.8080, 0.6590, 0.7828,  ..., 0.9014, 0.6813, 0.7584]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the pseudo-posterior, which is calculated after convergence of the messages above:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8080078959465027,\n",
       " 0.6589590311050415,\n",
       " 0.7827509045600891,\n",
       " 0.8446911573410034,\n",
       " 0.8253642916679382,\n",
       " 0.8066179156303406,\n",
       " 0.8760326504707336,\n",
       " 0.8127508163452148,\n",
       " 0.8029091358184814,\n",
       " 0.773499071598053,\n",
       " 0.8895395398139954,\n",
       " 0.6910153031349182,\n",
       " 0.8888853192329407,\n",
       " 0.813069760799408,\n",
       " 0.6996698975563049,\n",
       " 0.8476549983024597,\n",
       " 0.9170989394187927,\n",
       " 0.8156989216804504,\n",
       " 0.7129332423210144,\n",
       " 0.692522406578064,\n",
       " 0.8163334727287292,\n",
       " 0.718087911605835,\n",
       " 0.8134343028068542,\n",
       " 0.8404116630554199,\n",
       " 0.7938787937164307,\n",
       " 0.8709847331047058,\n",
       " 0.8363146781921387,\n",
       " 0.6452261209487915,\n",
       " 0.718925952911377,\n",
       " 0.8686188459396362,\n",
       " 0.8299380540847778,\n",
       " 0.8682169318199158,\n",
       " 0.8374934792518616,\n",
       " 0.8109815716743469,\n",
       " 0.8171431422233582,\n",
       " 0.7192045450210571,\n",
       " 0.6047594547271729,\n",
       " 0.5446777939796448,\n",
       " 0.7672617435455322,\n",
       " 0.8212751746177673,\n",
       " 0.6985032558441162,\n",
       " 0.6581689119338989,\n",
       " 0.7104586362838745,\n",
       " 0.8273010849952698,\n",
       " 0.6729130148887634,\n",
       " 0.7186986804008484,\n",
       " 0.8335407972335815,\n",
       " 0.7743744254112244,\n",
       " 0.4395231604576111,\n",
       " 0.8837853670120239,\n",
       " 0.833339273929596,\n",
       " 0.7083777189254761,\n",
       " 0.7754430174827576,\n",
       " 0.8070788979530334,\n",
       " 0.8066083788871765,\n",
       " 0.7357063293457031,\n",
       " 0.8364134430885315,\n",
       " 0.5556055903434753,\n",
       " 0.8227777481079102,\n",
       " 0.7746920585632324,\n",
       " 0.8280952572822571,\n",
       " 0.7741692662239075,\n",
       " 0.919754147529602,\n",
       " 0.8331922292709351,\n",
       " 0.7863110899925232,\n",
       " 0.7371833324432373,\n",
       " 0.8327861428260803,\n",
       " 0.8887667059898376,\n",
       " 0.8523324728012085,\n",
       " 0.8669995069503784,\n",
       " 0.8626105785369873,\n",
       " 0.8431335091590881,\n",
       " 0.8194299936294556,\n",
       " 0.8064723610877991,\n",
       " 0.7058414220809937,\n",
       " 0.7933580279350281,\n",
       " 0.6914851665496826,\n",
       " 0.7616048455238342,\n",
       " 0.8079637289047241,\n",
       " 0.8928748369216919,\n",
       " 0.80482417345047,\n",
       " 0.7922834157943726,\n",
       " 0.7621486186981201,\n",
       " 0.8529379963874817,\n",
       " 0.8125888705253601,\n",
       " 0.807386040687561,\n",
       " 0.8117252588272095,\n",
       " 0.8205104470252991,\n",
       " 0.6336696743965149,\n",
       " 0.8307944536209106,\n",
       " 0.8555696606636047,\n",
       " 0.8062644600868225,\n",
       " 0.8827274441719055,\n",
       " 0.8721684217453003,\n",
       " 0.7960276007652283,\n",
       " 0.8700042963027954,\n",
       " 0.8368500471115112,\n",
       " 0.9041032791137695,\n",
       " 0.6813071966171265,\n",
       " 0.7583566308021545]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_bayes = []\n",
    "\n",
    "for j in range(N):\n",
    "    \n",
    "    M_set = torch.where(encoding == j)[0]\n",
    "       \n",
    "    m_j = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "    m_bayes.append(m_j.item())\n",
    "            \n",
    "m_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(m_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering all message iterations on functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_to_mhat(N, M, J_, m, beta, encoding):\n",
    "    \n",
    "    for mu in range(M):\n",
    "        for j in range(N):\n",
    "                     \n",
    "            # Keep only L(mu) which a are different of j\n",
    "            index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "            L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "            # Message update        \n",
    "            m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)\n",
    "            \n",
    "    return m_hat\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def mhat_to_m(N, M, m_hat, beta_prior, encoding):\n",
    "    \n",
    "    for j in range(N):\n",
    "        for mu in range(M):\n",
    "        \n",
    "            M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "            M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "        \n",
    "            m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)\n",
    "            \n",
    "    return m\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def m_bayes(N, m_hat_final, beta_prior, encoding):\n",
    "    \n",
    "    m_bayes = []\n",
    "\n",
    "    for j in range(N):\n",
    "           \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "       \n",
    "        m_j = torch.tanh(torch.take(np.arctanh(m_hat_final[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "        m_bayes.append(m_j.item())\n",
    "        \n",
    "    return m_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a function for the iterative decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP_LDPC(N, M, J_, beta, beta_prior, encoding, message, num_it= 100, verbose= 1):\n",
    "     \n",
    "    m_hat = torch.rand(M, N)\n",
    "    m = torch.empty(M, N)\n",
    "    \n",
    "    for j in range(num_it):\n",
    "        \n",
    "        print('--it = %d' % j)\n",
    "              \n",
    "        m = mhat_to_m(N, M, m_hat, beta_prior, encoding)\n",
    "        m_hat = m_to_mhat(N, M, J_, m, beta, encoding)\n",
    "        \n",
    "        ## Monitoring performace\n",
    "        if (j % verbose) == 0:\n",
    "            m_ = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "            print('overlap= ', torch.dot(message, torch.Tensor(np.sign(m_))).item() / N)\n",
    "        \n",
    "    m_final = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "    \n",
    "    return np.sign(m_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--it = 0\n",
      "overlap=  0.64\n",
      "--it = 1\n",
      "overlap=  0.66\n",
      "--it = 2\n",
      "overlap=  0.7\n",
      "--it = 3\n",
      "overlap=  0.7\n",
      "--it = 4\n",
      "overlap=  0.7\n",
      "--it = 5\n",
      "overlap=  0.68\n",
      "--it = 6\n",
      "overlap=  0.68\n",
      "--it = 7\n",
      "overlap=  0.68\n",
      "--it = 8\n",
      "overlap=  0.68\n",
      "--it = 9\n",
      "overlap=  0.68\n",
      "--it = 10\n",
      "overlap=  0.68\n",
      "--it = 11\n",
      "overlap=  0.68\n",
      "--it = 12\n",
      "overlap=  0.68\n",
      "--it = 13\n",
      "overlap=  0.68\n",
      "--it = 14\n",
      "overlap=  0.68\n",
      "--it = 15\n",
      "overlap=  0.68\n",
      "--it = 16\n",
      "overlap=  0.68\n",
      "--it = 17\n",
      "overlap=  0.68\n",
      "--it = 18\n",
      "overlap=  0.68\n",
      "--it = 19\n",
      "overlap=  0.68\n"
     ]
    }
   ],
   "source": [
    "opt_dec_Bayes = BP_LDPC(N, M, J_, beta, beta_prior, encoding, message[0], num_it= 20, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. It still slow (we will worrie about this latter), but it appears to work.\n",
    "\n",
    "Observe that we arbitrarily consider a certain `num_it`. Naturally, a important question remains: how do we monitor the convergence of BP?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
