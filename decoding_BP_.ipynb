{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding with Belief Propagation - Working to take some for loops out of the game\n",
    "\n",
    "We want to iterate the following the following equations:\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu } \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$  \n",
    "\n",
    "which are Eqs.(96) from the paper [Low-density parity-check codesâ€”A statistical physics perspective](https://www.sciencedirect.com/science/article/pii/S1076567002800180), by R. Vicente, D. Saad and Y. Kabashima.\n",
    "\n",
    "The function $ \\beta(x) $ is the Nishimori temperature,\n",
    "\n",
    "$$ \\beta(x) = \\frac{1}{2} \\log \\left(  \\frac{1- \\rho}{\\rho}  \\right) \\; \\; .  $$\n",
    "\n",
    "The quantity $\\rho$ is the flip probablility of the noisy channel (BSC),\n",
    "\n",
    "$$ P ( J | J^{(0)} ) = (1 - \\rho) \\delta_{J, J^{(0)} } + \\rho \\delta_{J, -J^{(0)} }   \\; \\; , $$\n",
    "\n",
    "whereas the prior distribution for each meassage bit is assumed to be\n",
    "\n",
    "$$ P ( S_j ) = (1 - \\rho_\\xi) \\delta_{+1, S_j }  +  \\rho_\\xi \\delta_{-1, S_j }  \\; \\; .  $$\n",
    "\n",
    "The object $ {\\cal L} (\\mu)$ represents the set of $K$ non-zero elements on the row $\\mu$ of the code generator matrix ${\\cal G}$ (the one which adds redundancy), \n",
    "\n",
    "$$  {\\cal L} (\\mu) = \\langle i_1, i_2, ..., i_K \\rangle  \\; \\; .  $$\n",
    "\n",
    "The are $C$ non-zero elements per column on the matrix ${\\cal G}$:\n",
    "\n",
    "$$ \\sum_{\\mu : j \\in {\\cal L}(\\mu)} i_j = C \\; \\; ; \\; \\; \\forall j = 1, ..., K \\; \\; . $$\n",
    "\n",
    "The object $ {\\cal M} (j)$ represents the set of all index sets that contain $j$.\n",
    "\n",
    "After convergence of the iterative procedure, we can calculate the pseudo-posterior:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$\n",
    "from which the Bayes optimal estimate is obtained\n",
    "$$ \\hat{\\xi}_j  = sign( m_j ) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the variables of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message lengh\n",
    "N = 10\n",
    "\n",
    "# Codeword lengh\n",
    "M = 20\n",
    "\n",
    "# Non-zero elements per row of the generation matrix\n",
    "K = 4\n",
    "\n",
    "# Number of messages\n",
    "n = 15\n",
    "\n",
    "# Noisy channel\n",
    "p = 0.5\n",
    "beta = 0.5*np.log( (1 - p) / p)\n",
    "\n",
    "# Message prior\n",
    "p_prior = 0.1\n",
    "beta_prior = 0.5*np.log( (1 - p_prior) / p_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating messages\n",
    "\n",
    "Each message is a $N$ dimensional vector. Generate a set of $n$ messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand([n, N])\n",
    "\n",
    "#### -1 with probability p_prior\n",
    "#### +1 with probability 1 - p_prior\n",
    "\n",
    "message = 2* (random > p_prior).float() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(message)\n",
    "message.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Each message is encoded to a high dimensional vector ${\\bf J}^{(0)} \\in \\{ \\pm 1  \\}^M$ defined as \n",
    "\n",
    "$$   J^{(0)}_{\\langle i_1, i_2, ...., i_K \\rangle} = \\xi_1 \\xi_ 2 ... \\xi_K  \\; \\; ,$$\n",
    "\n",
    "where $M$ sets of $K \\in [ 1, ..., N]$ indexes are randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.randint(0, N, [M, K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `encoding`, we construct the encoded message ${\\bf J}^{(0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "J0 = torch.take(message[0], encoding).prod(dim=1)\n",
    "J0 = J0.unsqueeze(0)\n",
    "\n",
    "# Loop over all messages\n",
    "for j in range(1, message.shape[0]):\n",
    "    \n",
    "    J0_ = torch.take(message[j], encoding).prod(dim=1)\n",
    "    J0_ = J0_.unsqueeze(0)\n",
    "    \n",
    "    J0 = torch.cat((J0, J0_), dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
      "         -1., -1.,  1., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
      "         -1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
      "          1.,  1., -1., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
      "         -1., -1.,  1.,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
      "          1.,  1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
      "         -1., -1.,  1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "          1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
      "          1.,  1.,  1.,  1., -1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 20])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J0)\n",
    "J0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupted version of the encoded set of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = torch.rand(J.shape)\n",
    "\n",
    "## Flip tensor: -1 with probability p\n",
    "flip = 2*(random <= p).float() - 1\n",
    "\n",
    "## J corrupted version: element wise multiplication\n",
    "J = torch.mul(J0, flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
      "          1., -1., -1.,  1., -1., -1.],\n",
      "        [ 1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "         -1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
      "         -1., -1., -1., -1., -1., -1.],\n",
      "        [-1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
      "          1., -1., -1., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
      "          1.,  1.,  1., -1., -1.,  1.],\n",
      "        [-1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,\n",
      "         -1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
      "          1., -1., -1.,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
      "         -1., -1., -1.,  1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
      "         -1., -1., -1., -1.,  1.,  1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
      "         -1., -1., -1.,  1., -1., -1.],\n",
      "        [-1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
      "         -1., -1.,  1.,  1.,  1., -1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
      "          1.,  1.,  1.,  1., -1., -1.],\n",
      "        [-1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,\n",
      "         -1., -1., -1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
      "         -1., -1., -1.,  1., -1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 20])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating BP equations for one message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus in one received message to iterate the belief propagation equations.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)    \\right)   \\; \\; ,  $$  \n",
    "\n",
    "with $j = 1, ..., N$ and $\\mu = 1, ..., M$.\n",
    "\n",
    "We cal this message `J_`. We will worry later about a loop over all the received messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
      "         1., -1., -1.,  1., -1., -1.])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "J_ = J[0]\n",
    "print(J_)\n",
    "print(J_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialization of the beliefs $m_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8490, 0.7651, 0.4813, 0.4422, 0.4468, 0.5893, 0.6805, 0.3218, 0.1575,\n",
       "         0.6146],\n",
       "        [0.5140, 0.8164, 0.9899, 0.2906, 0.3551, 0.5489, 0.0058, 0.7731, 0.5287,\n",
       "         0.6275],\n",
       "        [0.7086, 0.9687, 0.7248, 0.6376, 0.1314, 0.1247, 0.0086, 0.3216, 0.9359,\n",
       "         0.3480],\n",
       "        [0.2044, 0.4443, 0.5222, 0.2857, 0.8737, 0.5808, 0.0553, 0.9413, 0.4699,\n",
       "         0.3736],\n",
       "        [0.9157, 0.9591, 0.3256, 0.3243, 0.8502, 0.6642, 0.4659, 0.5185, 0.3577,\n",
       "         0.3249],\n",
       "        [0.4693, 0.4397, 0.9608, 0.1113, 0.8198, 0.4708, 0.5807, 0.9098, 0.5703,\n",
       "         0.3885],\n",
       "        [0.4464, 0.8441, 0.5228, 0.5068, 0.4439, 0.5191, 0.4601, 0.9782, 0.0131,\n",
       "         0.5136],\n",
       "        [0.6102, 0.6242, 0.2763, 0.1424, 0.3031, 0.1346, 0.1362, 0.0520, 0.7587,\n",
       "         0.5749],\n",
       "        [0.0355, 0.0157, 0.3879, 0.9603, 0.2527, 0.3132, 0.2789, 0.3422, 0.3664,\n",
       "         0.8268],\n",
       "        [0.5327, 0.0029, 0.2150, 0.6878, 0.3713, 0.2700, 0.1526, 0.3893, 0.2898,\n",
       "         0.9070],\n",
       "        [0.0270, 0.1917, 0.6796, 0.7380, 0.4756, 0.8263, 0.4480, 0.0304, 0.5464,\n",
       "         0.2474],\n",
       "        [0.1605, 0.1147, 0.5459, 0.4117, 0.9608, 0.7403, 0.8045, 0.0262, 0.6496,\n",
       "         0.0468],\n",
       "        [0.5019, 0.2613, 0.7334, 0.2727, 0.7918, 0.3992, 0.8998, 0.3459, 0.9858,\n",
       "         0.5821],\n",
       "        [0.5801, 0.0825, 0.5880, 0.3221, 0.6300, 0.3815, 0.4082, 0.4409, 0.8999,\n",
       "         0.2670],\n",
       "        [0.1514, 0.5118, 0.8231, 0.5656, 0.1879, 0.6733, 0.1423, 0.1145, 0.6493,\n",
       "         0.1651],\n",
       "        [0.6284, 0.4549, 0.5109, 0.9447, 0.6672, 0.4039, 0.8358, 0.3825, 0.3806,\n",
       "         0.1401],\n",
       "        [0.6207, 0.1416, 0.3722, 0.7861, 0.1233, 0.4265, 0.1618, 0.4219, 0.2293,\n",
       "         0.0441],\n",
       "        [0.6592, 0.7555, 0.3171, 0.2343, 0.2269, 0.2334, 0.0064, 0.0150, 0.2222,\n",
       "         0.2614],\n",
       "        [0.8841, 0.9239, 0.4475, 0.8056, 0.3165, 0.6147, 0.4117, 0.9516, 0.6468,\n",
       "         0.6410],\n",
       "        [0.0766, 0.6926, 0.2567, 0.9203, 0.5470, 0.6770, 0.5963, 0.9155, 0.0492,\n",
       "         0.4513]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an empty tensor to represent $\\hat{m}_{\\mu l}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat = torch.empty(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2205e-20,  4.5792e-41,  6.2205e-20,  4.5792e-41, -7.0023e+02,\n",
       "          3.0927e-41, -7.0023e+02,  3.0927e-41,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,         nan,         nan,  0.0000e+00,\n",
       "          0.0000e+00,  1.4013e-45,  7.7312e-01,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.2612e-44,  0.0000e+00, -2.1237e+26,  4.5790e-41, -1.1084e+02,\n",
       "          3.0927e-41,         nan,         nan,  0.0000e+00,  0.0000e+00],\n",
       "        [        nan,         nan, -2.1167e+26,  4.5790e-41,  1.4013e-45,\n",
       "          5.8076e-01,  1.0089e-43,  0.0000e+00,  9.8091e-45,  0.0000e+00],\n",
       "        [-2.1237e+26,  4.5790e-41, -1.1085e+02,  3.0927e-41,  0.0000e+00,\n",
       "          0.0000e+00,  1.4013e-45,  0.0000e+00,  1.4013e-45,  0.0000e+00],\n",
       "        [-2.1167e+26,  4.5790e-41,  1.4013e-45,  1.1133e-01,  2.0179e-43,\n",
       "          0.0000e+00,  4.2039e-45,  0.0000e+00, -2.1237e+26,  4.5790e-41],\n",
       "        [-1.1085e+02,  3.0927e-41,  0.0000e+00,  0.0000e+00,  1.4013e-45,\n",
       "          0.0000e+00,  1.4013e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.4013e-45,  0.0000e+00,  3.0268e-43,  0.0000e+00,  1.2612e-44,\n",
       "          0.0000e+00, -2.1237e+26,  4.5790e-41, -1.1085e+02,  3.0927e-41],\n",
       "        [ 0.0000e+00,  0.0000e+00,  5.6052e-45,  0.0000e+00,  1.4013e-45,\n",
       "          0.0000e+00,  1.4013e-44,  0.0000e+00,  1.4013e-45,  8.2679e-01],\n",
       "        [ 4.0357e-43,  0.0000e+00,  1.2612e-44,  0.0000e+00, -2.1237e+26,\n",
       "          4.5790e-41, -1.1085e+02,  3.0927e-41,  7.0065e-45,  0.0000e+00],\n",
       "        [ 5.6052e-45,  0.0000e+00,  1.4013e-45,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  1.4013e-45,  3.0398e-02,  5.0447e-43,  0.0000e+00],\n",
       "        [ 1.2612e-44,  0.0000e+00, -2.1237e+26,  4.5790e-41, -1.1085e+02,\n",
       "          3.0927e-41,  8.0450e-01,  2.6151e-02,  8.4078e-45,  0.0000e+00],\n",
       "        [ 4.2039e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4013e-45,\n",
       "          3.9918e-01,  6.0536e-43,  0.0000e+00,  1.5414e-44,  0.0000e+00],\n",
       "        [-2.1237e+26,  4.5790e-41, -1.1085e+02,  3.0927e-41,  2.8026e-44,\n",
       "          0.0000e+00,  8.4078e-45,  0.0000e+00,  4.2039e-45,  0.0000e+00],\n",
       "        [ 1.5136e-01,  5.1178e-01,  1.4013e-45,  5.6558e-01,  7.0625e-43,\n",
       "          0.0000e+00,  1.4013e-44,  0.0000e+00, -2.1237e+26,  4.5790e-41],\n",
       "        [-1.1085e+02,  3.0927e-41,  5.1092e-01,  9.4470e-01,  6.6718e-01,\n",
       "          4.0392e-01,  8.3584e-01,  3.8250e-01,  3.8057e-01,  1.4013e-01],\n",
       "        [ 1.4013e-45,  1.4159e-01,  3.7223e-01,  7.8612e-01,  1.2328e-01,\n",
       "          4.2648e-01,  1.6185e-01,  4.2192e-01,  2.2927e-01,  4.4062e-02],\n",
       "        [ 6.5924e-01,  7.5551e-01,  3.1708e-01,  2.3432e-01,  2.2691e-01,\n",
       "          2.3335e-01,  6.4107e-03,  1.5019e-02,  2.2221e-01,  2.6137e-01],\n",
       "        [ 8.8414e-01,  9.2393e-01,  4.4755e-01,  8.0561e-01,  3.1651e-01,\n",
       "          6.1472e-01,  4.1166e-01,  9.5160e-01,  6.4680e-01,  6.4102e-01],\n",
       "        [ 7.6585e-02,  6.9259e-01,  2.5671e-01,  9.2030e-01,  5.4701e-01,\n",
       "          6.7698e-01,  5.9630e-01,  9.1545e-01,  4.9210e-02,  4.5134e-01]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate $\\hat{m}_{\\mu j}$.\n",
    "\n",
    "$$ \\hat{m}_{\\mu j} = \\tanh ( \\beta(\\rho) J_\\mu ) \\prod_{ l \\in {\\cal L} (\\mu) \\backslash j } m_{\\mu l}  \\; \\; ,  $$ \n",
    "\n",
    "This first implementation has two `for` loops. This is potentially harmful if one cares about efficienty. We obviously do, but since we are just beginning, lets go on like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mu in range(M):\n",
    "    for j in range(N):\n",
    "          \n",
    "        # Keep only L(mu) which a are different of j\n",
    "        index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "        L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "        # Message update        \n",
    "        m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8, 0, 6],\n",
       "        [4, 5, 2, 6],\n",
       "        [8, 8, 4, 8],\n",
       "        [2, 6, 6, 1],\n",
       "        [5, 1, 1, 8],\n",
       "        [5, 2, 8, 3],\n",
       "        [3, 9, 2, 8],\n",
       "        [0, 2, 0, 3],\n",
       "        [6, 9, 5, 6],\n",
       "        [7, 2, 3, 8],\n",
       "        [6, 9, 1, 5],\n",
       "        [3, 6, 5, 4],\n",
       "        [5, 7, 8, 6],\n",
       "        [8, 6, 1, 8],\n",
       "        [4, 2, 9, 1],\n",
       "        [2, 1, 5, 4],\n",
       "        [7, 3, 5, 2],\n",
       "        [5, 3, 4, 0],\n",
       "        [3, 7, 4, 6],\n",
       "        [8, 8, 1, 3]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [False, False,  True, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True, False,  True],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True, False,  True],\n",
       "        [False,  True,  True, False],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [False, False,  True,  True]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding != 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66, 2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.take(encoding, torch.nonzero(encoding != 8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 6, 4, 5, 2, 6, 4, 2, 6, 6, 1, 5, 1, 1, 5, 2, 3, 3, 9, 2, 0, 2, 0, 3,\n",
       "        6, 9, 5, 6, 7, 2, 3, 6, 9, 1, 5, 3, 6, 5, 4, 5, 7, 6, 6, 1, 4, 2, 9, 1,\n",
       "        2, 1, 5, 4, 7, 3, 5, 2, 5, 3, 4, 0, 3, 7, 4, 6, 1, 3])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[encoding != 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4])\n",
      "torch.Size([66, 2])\n"
     ]
    }
   ],
   "source": [
    "print((encoding != 8).shape)\n",
    "print(torch.nonzero(encoding != 8).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  1,  1,  1,  1,  2,  3,  3,  3,  3,  4,  4,  4,  5,  5,  5,  6,\n",
       "          6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9, 10, 10, 10, 10, 11,\n",
       "         11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16,\n",
       "         16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19]),\n",
       " tensor([2, 3, 0, 1, 2, 3, 2, 0, 1, 2, 3, 0, 1, 2, 0, 1, 3, 0, 1, 2, 0, 1, 2, 3,\n",
       "         0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 3, 1, 2, 0, 1, 2, 3,\n",
       "         0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(encoding != 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  0,  1,  1,  1,  1,  2,  3,  3,  3,  3,  4,  4,  4,  5,  5,  5,  6,\n",
       "          6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9, 10, 10, 10, 10, 11,\n",
       "         11, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16,\n",
       "         16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19]),\n",
       " tensor([2, 3, 0, 1, 2, 3, 2, 0, 1, 2, 3, 0, 1, 2, 0, 1, 3, 0, 1, 2, 0, 1, 2, 3,\n",
       "         0, 1, 2, 3, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 3, 1, 2, 0, 1, 2, 3,\n",
       "         0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 2, 3]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(encoding != 8, as_tuple= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8, 8, 0, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[8, 8, 0, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 8, 4, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 6, 1, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[8, 6, 1, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 8, 1, 3],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[8, 8, 1, 3],\n",
       "         [2, 6, 6, 1]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[torch.nonzero(encoding != 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.nonzero(encoding != 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8, 8, 0, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[8, 8, 0, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 5, 2, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 8, 4, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 6, 6, 1],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 1, 1, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 2, 8, 3],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 9, 2, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[0, 2, 0, 3],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 5, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[7, 2, 3, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[6, 9, 1, 5],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[3, 6, 5, 4],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 7, 8, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 6, 1, 8],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[8, 6, 1, 8],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[4, 2, 9, 1],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[2, 1, 5, 4],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[7, 3, 5, 2],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[5, 3, 4, 0],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [8, 8, 0, 6]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [4, 5, 2, 6]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[3, 7, 4, 6],\n",
       "         [2, 6, 6, 1]],\n",
       "\n",
       "        [[8, 8, 1, 3],\n",
       "         [8, 8, 4, 8]],\n",
       "\n",
       "        [[8, 8, 1, 3],\n",
       "         [2, 6, 6, 1]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
       "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement:\n",
    "\n",
    "$$ m_{\\mu j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) \\backslash \\mu} \\ \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right) + \\beta(\\rho_\\xi)     \\right)  \\; \\; ,  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(N):\n",
    "    \n",
    "    for mu in range(M):\n",
    "        \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "        M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "        \n",
    "        m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        ...,\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
       "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the pseudo-posterior, which is calculated after convergence of the messages above:\n",
    "\n",
    "$$ m_{j} = \\tanh \\left(  \\sum_{ \\nu \\in {\\cal M} (j) } \\tanh^{-1} \\left(  \\hat{m}_{\\nu j}  \\right)    + \\beta(\\rho_\\xi)  \\right)   \\; \\; ,  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929,\n",
       " 0.800000011920929]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_bayes = []\n",
    "\n",
    "for j in range(N):\n",
    "    \n",
    "    M_set = torch.where(encoding == j)[0]\n",
    "       \n",
    "    m_j = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "    m_bayes.append(m_j.item())\n",
    "            \n",
    "m_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(m_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering all message iterations on functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_to_mhat(N, M, J_, m, beta, encoding):\n",
    "    \n",
    "    m_hat = torch.empty(M, N)\n",
    "    \n",
    "    for mu in range(M):\n",
    "        for j in range(N):\n",
    "                     \n",
    "            # Keep only L(mu) which a are different of j\n",
    "            index_no_j = torch.nonzero(encoding[mu] != j).squeeze()\n",
    "            L_no_j = encoding[mu][index_no_j]\n",
    "        \n",
    "            # Message update        \n",
    "            m_hat[mu, j] = torch.tanh( beta* J_[mu])*torch.take(m[mu], L_no_j).prod(dim=0)\n",
    "            \n",
    "    return m_hat\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def mhat_to_m(N, M, m_hat, beta_prior, encoding):\n",
    "    \n",
    "    m = torch.empty(M, N)\n",
    "    \n",
    "    for j in range(N):\n",
    "        \n",
    "        for mu in range(M):\n",
    "            \n",
    "            M_set = torch.where(encoding == j)[0]\n",
    "        \n",
    "            M_set_no_mu = M_set[torch.nonzero(M_set != mu).squeeze()]\n",
    "            \n",
    "            m[mu, j] = torch.tanh(torch.take(np.arctanh(m_hat[:, j]), M_set_no_mu).sum() + beta_prior)\n",
    "            \n",
    "    return m\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "def m_bayes(N, m_hat_final, beta_prior, encoding):\n",
    "    \n",
    "    m_bayes = []\n",
    "\n",
    "    for j in range(N):\n",
    "           \n",
    "        M_set = torch.where(encoding == j)[0]\n",
    "  \n",
    "        m_j = torch.tanh(torch.take(np.arctanh(m_hat_final[:, j]), M_set).sum() + beta_prior)\n",
    "    \n",
    "        m_bayes.append(m_j.item())\n",
    "        \n",
    "    return m_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a function for the iterative decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP_LDPC(N, M, J_, beta, beta_prior, encoding, message, num_it= 100, verbose= 1):\n",
    "     \n",
    "    m_hat = torch.rand(M, N)\n",
    "    m = torch.empty(M, N)\n",
    "    \n",
    "    for j in range(num_it):\n",
    "        \n",
    "        print('--it = %d' % j)\n",
    "              \n",
    "        m = mhat_to_m(N, M, m_hat, beta_prior, encoding)\n",
    "        m_hat = m_to_mhat(N, M, J_, m, beta, encoding)\n",
    "        \n",
    "        ## Monitoring performace\n",
    "        if (j % verbose) == 0:\n",
    "            m_ = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "            print('overlap= ', torch.dot(message, torch.Tensor(np.sign(m_))).item() / N)\n",
    "        \n",
    "    m_final = m_bayes(N, m_hat, beta_prior, encoding)\n",
    "    \n",
    "    return np.sign(m_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--it = 0\n",
      "overlap=  0.76\n",
      "--it = 1\n",
      "overlap=  0.76\n",
      "--it = 2\n",
      "overlap=  0.76\n",
      "--it = 3\n",
      "overlap=  0.76\n",
      "--it = 4\n",
      "overlap=  0.76\n",
      "--it = 5\n",
      "overlap=  0.76\n",
      "--it = 6\n",
      "overlap=  0.76\n",
      "--it = 7\n",
      "overlap=  0.76\n",
      "--it = 8\n",
      "overlap=  0.76\n",
      "--it = 9\n",
      "overlap=  0.76\n",
      "--it = 10\n",
      "overlap=  0.76\n",
      "--it = 11\n",
      "overlap=  0.76\n",
      "--it = 12\n",
      "overlap=  0.76\n",
      "--it = 13\n",
      "overlap=  0.76\n",
      "--it = 14\n",
      "overlap=  0.76\n",
      "--it = 15\n",
      "overlap=  0.76\n",
      "--it = 16\n",
      "overlap=  0.76\n",
      "--it = 17\n",
      "overlap=  0.76\n",
      "--it = 18\n",
      "overlap=  0.76\n",
      "--it = 19\n",
      "overlap=  0.76\n"
     ]
    }
   ],
   "source": [
    "opt_dec_Bayes = BP_LDPC(N, M, J_, beta, beta_prior, encoding, message[0], num_it= 20, verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. It is still slow (we will worrie about this latter), but it appears to work.\n",
    "\n",
    "Observe that we arbitrarily consider a certain `num_it`. Naturally, a important question remains: how do we monitor the convergence of BP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test on previous codes and messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message lengh\n",
    "N = 250\n",
    "\n",
    "# Codeword lengh\n",
    "M = 500\n",
    "\n",
    "# Non-zero elements per row of the generation matrix\n",
    "K = 4\n",
    "\n",
    "# Number of messages\n",
    "n = 100\n",
    "\n",
    "# Noisy channel\n",
    "p = 0.3\n",
    "beta = 0.5*np.log( (1 - p) / p)\n",
    "\n",
    "# Message prior\n",
    "p_prior = 0.01\n",
    "beta_prior = 0.5*np.log( (1 - p_prior) / p_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.load('codes/code_N_250_M_500_K_4_p_03_p_prior_001_j_0.pt')\n",
    "message = torch.load('codes/message_N_250_M_500_K_4_p_03_p_prior_001.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        ...,\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 250])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(message)\n",
    "message.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we made a mistake saving the codes. We have actually saved the messages. That is a problem. For now, let us generate a new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 94,  21, 204, 242],\n",
       "        [162,  49, 100, 151],\n",
       "        [230, 198, 123,  57],\n",
       "        ...,\n",
       "        [ 87, 176,   1, 211],\n",
       "        [ 99,  72,  51,  91],\n",
       "        [248, 145, 105, 154]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding  = torch.randint(0, N, [M, K])\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing\n",
    "J0 = torch.take(message[0], encoding).prod(dim=1)\n",
    "J0 = J0.unsqueeze(0)\n",
    "\n",
    "for j in range(1, message.shape[0]):\n",
    "    \n",
    "    J0_ = torch.take(message[j], encoding).prod(dim=1)\n",
    "    J0_ = J0_.unsqueeze(0)\n",
    "    \n",
    "    J0 = torch.cat((J0, J0_), dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 500])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J0)\n",
    "J0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrupted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J0.clone()\n",
    "\n",
    "random = torch.rand(J.shape)\n",
    "                      \n",
    "for j in range(J.shape[0]):\n",
    "    for k in range(J.shape[1]):\n",
    "          \n",
    "        if random[j, k] <= p:\n",
    "            J[j, k] = -J[j, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 500])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(J)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "500\n",
      "torch.Size([500, 4])\n",
      "torch.Size([100, 500])\n",
      "torch.Size([100, 250])\n"
     ]
    }
   ],
   "source": [
    "print(N)\n",
    "print(M)\n",
    "print(encoding.shape)\n",
    "print(J.shape)\n",
    "print(message.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_list = []\n",
    "\n",
    "for k in range(int(J.shape[0] / 10 )):\n",
    "    \n",
    "    print('----message %d' % k)\n",
    "       \n",
    "    opt_dec_Bayes = BP_LDPC(N, M, J[k], beta, beta_prior, encoding, message[k], num_it= 10, verbose= 1)\n",
    "    \n",
    "    overlap_list.append(opt_dec_Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "    list_.append([2, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0, 2, 4, 5, 3, 5]\n",
    "p = [98.8, 89, 5.9, 3.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, 4, 5, 3, 5]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, 4, 5, 3, 5], [98.8, 89, 5.9, 3.4]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 3, 5]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
